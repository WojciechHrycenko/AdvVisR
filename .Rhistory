# 5. Define a function to perform a score test for zero inflation and LT test
zero.test <- function(x) {
if(is.table(x)) {
if(length(dim(x)) > 1) stop("Input must be a 1-way table")
x <- rep(as.numeric(names(x)), unname(c(x)))
}
lambda <- mean(x)
p0_tilde <- exp(-lambda)
n0 <- sum(!(x > 0))
n <- length(x)
numerator <- (n0 - n * p0_tilde)^2
denominator <- n * p0_tilde * (1 - p0_tilde) - n * lambda * (p0_tilde^2)
stat <- numerator / denominator
pvalue <- pchisq(stat, df = 1, lower.tail = FALSE)
result <- list(statistic = stat, df = 1, p_value = pvalue)
cat(paste("Score test for zero inflation\n\n",
"\tChi-square =", round(stat, 5), "\n",
"\tdf = 1\n",
"\tp-value:", format.pval(pvalue), "\n"))
print(result)
}
lr_test <- function(model1, model2) {
lr_stat <- 2 * (logLik(model2)[1] - logLik(model1)[1])
df_diff <- attr(logLik(model2), "df") - attr(logLik(model1), "df")
p_val <- pchisq(lr_stat, df = df_diff, lower.tail = FALSE)
cat("LR test comparing models:\n")
cat("  Statistic =", round(lr_stat, 4), "\n")
cat("  Degrees of freedom =", df_diff, "\n")
cat("  p-value =", p_val, "\n\n")
return(list(statistic = lr_stat, df = df_diff, p.value = p_val))
}
# 6. Perform zero-inflation test on 'total_victims'
zero_test_result <- zero.test(df$total_victims)
p_val <- zero_test_result$p_value
cat("\n--- Interpretation of zero.test result ---\n")
if (p_val < 0.05) {
cat("Reject H0: Evidence of zero inflation beyond the Poisson model.\n")
cat("Consider zero-inflated models (ZIP or ZINB).\n")
} else {
cat("Fail to reject H0: Poisson model may be adequate.\n")
}
# 7. Fit a Poisson regression model
# Poisson
poisson_model <- glm(total_victims ~ road_surface + direction_of_travel + lighting + weather_1 + party_sex + party_age + party_sobriety + financial_responsibility + cellphone_in_use + party_race + party_count + hit_and_run + car_age + season + year + region + time_of_day + chp_beat_type + chp_vehicle_type_at_fault + party_safety_equipment_1 +   type_of_collision + county_location + population + motor_vehicle_involved_with + movement_preceding_collision, family = "poisson", data = df)
# Negative Binomial
nb_model <- glm.nb(total_victims ~ road_surface + direction_of_travel + lighting + weather_1 + party_sex + party_age + party_sobriety + financial_responsibility + cellphone_in_use + party_race  + party_count + hit_and_run + car_age + season + year + region + time_of_day + chp_beat_type + chp_vehicle_type_at_fault + party_safety_equipment_1 + type_of_collision + county_location + population + motor_vehicle_involved_with + movement_preceding_collision, data = df)
# ZIP
zip_model <- zeroinfl(total_victims ~ road_surface + direction_of_travel + lighting + weather_1 + party_sex + party_age + party_sobriety + financial_responsibility + cellphone_in_use + party_race + party_count + hit_and_run + car_age + season + year + region + time_of_day + chp_beat_type + chp_vehicle_type_at_fault + party_safety_equipment_1 + type_of_collision + county_location + population + motor_vehicle_involved_with + movement_preceding_collision, data = df, dist = "poisson")
# ZINB
zinb_model <- zeroinfl(total_victims ~ road_surface + direction_of_travel + lighting + weather_1 + party_sex + party_age + party_sobriety + financial_responsibility + cellphone_in_use + party_race + party_count + hit_and_run + car_age + season + year + region + time_of_day + chp_beat_type + chp_vehicle_type_at_fault + party_safety_equipment_1 + type_of_collision + county_location + population + motor_vehicle_involved_with + movement_preceding_collision, data = df, dist = "negbin")
# 11. Collect all models in a list for comparison
model_list <- list(
Poisson = poisson_model,
NegBin = nb_model,
ZIP = zip_model,
ZINB = zinb_model
)
# 12. Create a data frame summarizing AIC and log-likelihood for each model
model_comparison <- data.frame(
Model = names(model_list),
AIC = sapply(model_list, AIC),
LogLik = sapply(model_list, function(m) as.numeric(logLik(m)))
)
# 13. Sort the comparison table by AIC (lower is better)
model_comparison <- model_comparison[order(model_comparison$AIC), ]
cat("Poisson vs Negative Binomial:\n")
lr_test(poisson_model, nb_model)
cat("Poisson vs Zero-Inflated Poisson:\n")
lr_test(poisson_model, zip_model)
cat("Negative Binomial vs Zero-Inflated Negative Binomial:\n")
lr_test(nb_model, zinb_model)
cat("Zero-Inflated Poisson vs Zero-Inflated Negative Binomial:\n")
lr_test(zip_model, zinb_model)
# 14. Print the comparison results
print(model_comparison)
install.packages("modelsummary")
install.packages(c("cli", "collections", "curl", "data.table", "fs", "haven", "openssl", "parallelly", "pkgbuild", "promises", "ps", "recipes", "rlang", "rsconnect", "sparsevctrs", "utf8", "xfun"))
install.packages(c("cli", "collections", "data.table", "fs", "ps", "rlang", "xfun"))
install.packages("modelsummary")
knitr::opts_chunk$set(echo = TRUE)
options(repos = c(CRAN = "https://cloud.r-project.org"))
install.packages("lmtest")
library(DBI)
library(RSQLite)
library(dplyr)
library(tidyr)
library(knitr)
library(lubridate)
library(ggplot2)
library(pscl)
library(MASS)# for extracting year
library(pscl)
library(lmtest)
library(modelsummary)  # for table formatting
library(broom)
knitr::opts_chunk$set(echo = TRUE)
library(DBI)
library(RSQLite)
library(dplyr)
library(tidyr)
library(knitr)
library(lubridate)
library(ggplot2)
library(pscl)
library(MASS)# for extracting year
library(pscl)
library(lmtest)
library(modelsummary)  # for table formatting
library(broom)
install.packages(c("cli", "collections", "data.table", "fs", "ps", "rlang", "xfun"))
knitr::opts_chunk$set(echo = TRUE)
library(DBI)
library(RSQLite)
library(dplyr)
library(tidyr)
library(knitr)
library(lubridate)
library(ggplot2)
library(pscl)
library(MASS)# for extracting year
library(pscl)
library(lmtest)
library(modelsummary)  # for table formatting
library(broom)
install.packages(c("cli", "data.table", "rlang", "xfun"))
# For english messages
Sys.setenv(LANG = "en")
#---------------------------------------------------------#
#               Advanced Visualization in R               #
#                   Piotr Ćwiakowski                      #
#             Faculty of Economic Sciences UW             #
#                       Line plots                        #
#---------------------------------------------------------#
# Downloading packages
# install.packages('tidyverse')
# For english messages
Sys.setenv(LANG = "en")
# Upload packages to memory
library(ggplot2)
library(dplyr)
library(tidyr)
library(lubridate)
library(readr)
# import data
load('data/Walmart.Rdata')
#---------------------------------------------------------#
#               Advanced Visualization in R               #
#                   Piotr Ćwiakowski                      #
#             Faculty of Economic Sciences UW             #
#                       Line plots                        #
#---------------------------------------------------------#
# Downloading packages
# install.packages('tidyverse')
# For english messages
Sys.setenv(LANG = "en")
# Upload packages to memory
library(ggplot2)
library(dplyr)
library(tidyr)
library(lubridate)
library(readr)
# import data
load('Walmart.Rdata')
#---------------------------------------------------------#
#               Advanced Visualization in R               #
#                   Piotr Ćwiakowski                      #
#             Faculty of Economic Sciences UW             #
#                       Line plots                        #
#---------------------------------------------------------#
# Downloading packages
# install.packages('tidyverse')
# For english messages
Sys.setenv(LANG = "en")
# Upload packages to memory
library(ggplot2)
library(dplyr)
library(tidyr)
library(lubridate)
library(readr)
# import data
load('Walmart.Rdata')
# Read libs
library(tidyverse)
install.packages('qqplotr')
install.packages('tidyverse')
# Read libs
library(tidyverse)
#---------------------------------------------------------#
#               Advanced Visualization in R               #
#                    Piotr Ćwiakowski                     #
#             Faculty of Economic Sciences UW             #
#           Visualising statistical distribution          #
#---------------------------------------------------------#
# Set the working directory
setwd('...')
# Read database
spare.time <- read.csv("Data/free_time.csv", sep = ',')
#---------------------------------------------------------#
#               Advanced Visualization in R               #
#                    Piotr Ćwiakowski                     #
#             Faculty of Economic Sciences UW             #
#           Visualising statistical distribution          #
#---------------------------------------------------------#
# Set the working directory
setwd('...')
# Read database
spare.time <- read.csv("Data/free_time.csv", sep = ',')
#---------------------------------------------------------#
#               Advanced Visualization in R               #
#                    Piotr Ćwiakowski                     #
#             Faculty of Economic Sciences UW             #
#           Visualising statistical distribution          #
#---------------------------------------------------------#
# Set the working directory
setwd('...')
# Read database
spare.time <- read.csv("Data/free_time.csv", sep = ',')
source("C:/Users/wojci/Desktop/VisR/9_distribution.R")
#---------------------------------------------------------#
#               Advanced Visualization in R               #
#                    Piotr Ćwiakowski                     #
#             Faculty of Economic Sciences UW             #
#           Visualising statistical distribution          #
#---------------------------------------------------------#
# Set the working directory
setwd('...')
# Read database
spare.time <- read.csv("Data/free_time.csv", sep = ',')
#---------------------------------------------------------#
#               Advanced Visualization in R               #
#                    Piotr Ćwiakowski                     #
#             Faculty of Economic Sciences UW             #
#           Visualising statistical distribution          #
#---------------------------------------------------------#
# Set the working directory
# Read database
spare.time <- read.csv("Data/free_time.csv", sep = ',')
#---------------------------------------------------------#
#               Advanced Visualization in R               #
#                    Piotr Ćwiakowski                     #
#             Faculty of Economic Sciences UW             #
#           Visualising statistical distribution          #
#---------------------------------------------------------#
# Set the working directory
setwd('...')
# Read database
spare.time <- read.csv("free_time.csv", sep = ',')
#---------------------------------------------------------#
#               Advanced Visualization in R               #
#                    Piotr Ćwiakowski                     #
#             Faculty of Economic Sciences UW             #
#           Visualising statistical distribution          #
#---------------------------------------------------------#
# Set the working directory
setwd('...')
# Read database
spare.time <- read.csv("Data\free_time.csv", sep = ',')
#---------------------------------------------------------#
#               Advanced Visualization in R               #
#                    Piotr Ćwiakowski                     #
#             Faculty of Economic Sciences UW             #
#           Visualising statistical distribution          #
#---------------------------------------------------------#
# Set the working directory
setwd('...')
# Read database
spare.time <- read.csv("Data/free_time.csv", sep = ',')
# --- PLOT 7: CORRELATION PLOT ---
corr_matrix <- df %>%
select(Calories, Proteins, Fats, Carbs) %>%
cor(use = "complete.obs", method = "pearson")
library(tidyverse)
library(scales)
library(ggalluvial)
library(treemapify)
library(ggcorrplot)
# Load data
df <- read_csv("Data/Final_data_model.csv")
setwd(C:/Users/wojci/Desktop/VisR/AdvVisR)
setwd(Users/wojci/Desktop/VisR/AdvVisR)
setwd(C/Users/wojci/Desktop/VisR/AdvVisR)
setwd(C:/Users/wojci/Desktop/VisR/AdvVisR/)
setwd("C:/Users/wojci/Desktop/VisR/AdvVisR/")
library(tidyverse)
library(scales)
library(ggalluvial)
library(treemapify)
library(ggcorrplot)
# Load data
df <- read_csv("Data/Final_data_model.csv")
# Preprocessing
df <- df %>%
mutate(
`Difficulty Level` = factor(`Difficulty Level`, levels = c("Beginner", "Intermediate", "Advanced")),
Age_Group = cut(Age, breaks = c(0, 25, 35, 45, 100), labels = c("18-25", "26-35", "36-45", "45+")),
Burn_Category = case_when(
Calories_Burned < 500 ~ "Low Burn",
Calories_Burned < 1000 ~ "Med Burn",
TRUE ~ "High Burn"
)
)
# --- PLOT 1: SCALED SPIDER CHART ---
create_macro_radar <- function(data, group_col, plot_title) {
custom_limits <- tibble(
Nutrient = c("Proteins", "Fats", "Carbs"),
Max_Limit = c(200, 100, 400)
)
radar_data <- data %>%
group_by({{ group_col }}) %>%
summarise(
Proteins = mean(Proteins, na.rm = TRUE),
Fats = mean(Fats, na.rm = TRUE),
Carbs = mean(Carbs, na.rm = TRUE)
) %>%
pivot_longer(-{{ group_col }}, names_to = "Nutrient", values_to = "Value") %>%
left_join(custom_limits, by = "Nutrient") %>%
mutate(Value_Norm = Value / Max_Limit)
radar_coords <- radar_data %>%
mutate(
angle = case_when(
Nutrient == "Proteins" ~ 90 * pi / 180,
Nutrient == "Carbs"    ~ 330 * pi / 180,
Nutrient == "Fats"     ~ 210 * pi / 180
),
x = Value_Norm * cos(angle),
y = Value_Norm * sin(angle)
)
grid_levels <- c(0.25, 0.50, 0.75, 1.00)
grid_data <- expand_grid(level = grid_levels, angle = c(90, 330, 210) * pi / 180) %>%
mutate(x = level * cos(angle), y = level * sin(angle)) %>%
arrange(level, desc(angle)) %>%
group_by(level) %>%
slice(c(1:n(), 1))
axis_labels <- expand_grid(Nutrient = c("Proteins", "Fats", "Carbs"), level = grid_levels) %>%
left_join(custom_limits, by = "Nutrient") %>%
mutate(
Raw_Value = round(level * Max_Limit),
angle = case_when(
Nutrient == "Proteins" ~ 90 * pi / 180,
Nutrient == "Carbs"    ~ 330 * pi / 180,
Nutrient == "Fats"     ~ 210 * pi / 180
),
x = level * cos(angle),
y = level * sin(angle)
)
radar_closed <- radar_coords %>%
arrange({{ group_col }}, desc(angle)) %>%
group_by({{ group_col }}) %>%
slice(c(1:n(), 1))
ggplot() +
geom_polygon(data = grid_data, aes(x, y, group = level), fill = NA, color = "grey85", linetype = "dashed") +
geom_label(data = axis_labels, aes(x, y, label = Raw_Value), size = 2.5, color = "grey40", label.size = 0, fill = "white", alpha = 0.7) +
geom_polygon(data = radar_closed, aes(x, y, color = {{ group_col }}, fill = {{ group_col }}), alpha = 0.2, linewidth = 1) +
geom_point(data = radar_coords, aes(x, y, color = {{ group_col }}), size = 3) +
annotate("text", x = 0, y = 1.15, label = "Proteins\n(Max 200g)", fontface = "bold") +
annotate("text", x = 1.15 * cos(330*pi/180), y = 1.15 * sin(330*pi/180), label = "Carbs\n(Max 400g)", fontface = "bold", hjust = 0.5) +
annotate("text", x = 1.15 * cos(210*pi/180), y = 1.15 * sin(210*pi/180), label = "Fats\n(Max 100g)", fontface = "bold", hjust = 0.5) +
scale_color_brewer(palette = "Set1") +
scale_fill_brewer(palette = "Set1") +
coord_fixed() +
theme_void() +
theme(legend.position = "bottom") +
labs(title = plot_title)
}
print(create_macro_radar(df, `Difficulty Level`, "Macronutrient Profile by Difficulty (Scaled)"))
print(create_macro_radar(df, Workout_Type, "Macronutrient Profile by Workout Type (Scaled)"))
print(create_macro_radar(df, meal_type, "Macronutrient Profile by Meal Type (Scaled)"))
# --- PLOT 2: SCATTER PLOT (Training Efficiency) ---
p_scatter <- ggplot(df, aes(x = Avg_BPM, y = Calories_Burned, color = Workout_Type, size = `Session_Duration (hours)`)) +
geom_point(alpha = 0.6) +
scale_color_brewer(palette = "Set1") +
scale_size_continuous(range = c(1, 6), name = "Duration (hrs)") +
theme_minimal() +
theme(
legend.position = "right",
plot.title = element_text(face = "bold", size = 14),
axis.title = element_text(face = "bold")
) +
labs(
title = "Training Efficiency Analysis",
subtitle = "Impact of Intensity (BPM) on Calories Burned by Workout Type",
x = "Average Heart Rate (BPM)",
y = "Total Calories Burned"
)
print(p_scatter)
# --- PLOT 3: TREEMAP ---
tree_data <- df %>%
group_by(`Difficulty Level`, meal_type, cooking_method) %>%
summarise(Total_Calories = sum(Calories, na.rm = TRUE), .groups = "drop") %>%
filter(Total_Calories > 0) %>%
mutate(meal_type = factor(meal_type, levels = c("Breakfast", "Lunch", "Dinner", "Snack")))
p_tree <- ggplot(tree_data, aes(area = Total_Calories, fill = meal_type, label = cooking_method, subgroup = meal_type)) +
geom_treemap(layout = "squarified", color = "white", size = 2) +
geom_treemap_subgroup_border(colour = "white", size = 5) +
geom_treemap_subgroup_text(place = "topleft", grow = FALSE, colour = "#333333", fontface = "bold", size = 13, padding.x = grid::unit(3, "mm"), padding.y = grid::unit(3, "mm"), alpha = 1) +
geom_treemap_text(aes(label = paste0(cooking_method, "\n", format(round(Total_Calories, 0), big.mark = " ", trim = TRUE))), colour = "white", place = "centre", grow = FALSE, reflow = TRUE) +
facet_wrap(~ `Difficulty Level`) +
scale_fill_brewer(palette = "Set2") +
theme(
legend.position = "bottom",
plot.title = element_text(face = "bold", size = 16),
strip.text = element_text(face = "bold", size = 12)
) +
labs(
title = "Caloric Share by Meal Type & Preparation",
subtitle = "Size represents Total Calories | Ordered: Breakfast → Lunch → Dinner → Snack",
fill = "Meal Type"
)
print(p_tree)
# --- PLOT 4: ALLUVIAL PLOT ---
alluvial_data <- df %>%
mutate(
Experience_Cat = case_when(
Experience_Level <= 1.5 ~ "Beginner",
Experience_Level <= 2.5 ~ "Intermediate",
TRUE ~ "Advanced"
),
Experience_Cat = factor(Experience_Cat, levels = c("Beginner", "Intermediate", "Advanced")),
# Using created Burn_Category instead of potential csv column Burns_Calories_Bin for consistency
Burn_Category = factor(Burn_Category, levels = c("Low Burn", "Med Burn", "High Burn"))
) %>%
count(Experience_Cat, Workout_Type, Burn_Category)
p_alluvial <- ggplot(alluvial_data, aes(axis1 = Experience_Cat, axis2 = Workout_Type, axis3 = Burn_Category, y = n)) +
geom_alluvium(aes(fill = Experience_Cat), width = 1/12, alpha = 0.7, knot.pos = 0.4) +
geom_stratum(width = 1/12, fill = "grey95", color = "grey20") +
geom_text(stat = "stratum", aes(label = after_stat(stratum)), size = 3, fontface = "bold") +
scale_fill_brewer(palette = "Set1") +
theme_minimal() +
theme(
legend.position = "bottom",
axis.text.y = element_blank(),
axis.ticks = element_blank(),
panel.grid = element_blank()
) +
scale_x_discrete(limits = c("Experience", "Workout Type", "Burn Intensity"), expand = c(.05, .05)) +
labs(
title = "Workout Flows: Experience to Intensity",
subtitle = "Tracing the path from experience level to calorie burn efficiency",
fill = "Experience Level",
y = "Number of Sessions"
)
print(p_alluvial)
# --- PLOT 5: BMI HISTOGRAM ---
p_bmi <- ggplot(df, aes(x = BMI, fill = Gender)) +
geom_histogram(binwidth = 1, alpha = 0.6, position = "identity", color = "white") +
scale_fill_brewer(palette = "Set1") +
theme_minimal() +
theme(legend.position = "top", plot.title = element_text(face = "bold", size = 14)) +
labs(title = "BMI Distribution by Gender", x = "BMI Value", y = "Number of People")
print(p_bmi)
# --- PLOT 6: BAR CHART (TOP EXERCISES) ---
top_exercises <- df %>%
count(Name_of_Exercise, sort = TRUE) %>%
slice_head(n = 6) %>%
pull(Name_of_Exercise)
bar_data <- df %>%
filter(Name_of_Exercise %in% top_exercises) %>%
mutate(Freq_Day = round(`Workout_Frequency (days/week)`, 0))
p_freq_exercise <- ggplot(bar_data, aes(x = factor(Freq_Day), fill = Name_of_Exercise)) +
geom_bar(color = "black", alpha = 0.8, width = 0.7, show.legend = FALSE) +
facet_wrap(~ Name_of_Exercise, scales = "free_y") +
scale_fill_brewer(palette = "Dark2") +
theme_minimal() +
theme(
plot.title = element_text(face = "bold", size = 16),
strip.text = element_text(face = "bold", size = 12),
panel.grid.minor = element_blank()
) +
labs(
title = "Workout Frequency by Exercise Name (Top 6)",
subtitle = "Distribution of weekly training days for most popular exercises",
x = "Days per Week",
y = "Number of People"
)
print(p_freq_exercise)
# --- PLOT 7: CORRELATION PLOT ---
corr_matrix <- df %>%
select(Calories, Proteins, Fats, Carbs) %>%
cor(use = "complete.obs", method = "pearson")
p_corr <- ggcorrplot(corr_matrix, method = "circle", type = "lower", lab = TRUE, lab_size = 5,
colors = c("#E46726", "white", "#6D9EC1"),
title = "Correlation Matrix: Calories vs. Macronutrients",
ggtheme = theme_minimal()) +
theme(plot.title = element_text(face = "bold", size = 16), legend.position = "right")
print(p_corr)
# --- PLOT 8: GROUPED BAR CHART ---
grouped_data <- df %>%
mutate(Age_Group = cut(Age, breaks = c(-Inf, 30, 45, 60, Inf), labels = c("18-30", "31-45", "46-60", "60+"))) %>%
filter(!is.na(Age_Group)) %>%
group_by(Age_Group, Gender, Workout_Type) %>%
summarise(Count = n(), .groups = "drop_last") %>%
mutate(Percentage = Count / sum(Count) * 100)
p_grouped <- ggplot(grouped_data, aes(x = Age_Group, y = Percentage, fill = Gender)) +
geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.7, color = "black", alpha = 0.8) +
facet_wrap(~ Workout_Type) +
scale_y_continuous(labels = percent_format(scale = 1)) +
scale_fill_brewer(palette = "Set1") +
theme_minimal() +
theme(
legend.position = "top",
plot.title = element_text(face = "bold", size = 16),
strip.text = element_text(face = "bold", size = 12),
axis.text.x = element_text(angle = 0, hjust = 0.5)
) +
labs(
title = "Workout Preference by Age & Gender",
subtitle = "Percentage of workout types chosen within each demographic group",
x = "Age Group",
y = "Percentage share",
fill = "Gender"
)
print(p_grouped)
